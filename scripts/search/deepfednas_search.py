import torch
import pandas as pd
import numpy as np
import os
import sys
import json
from tqdm import tqdm
import time # Import the time module
from torch import multiprocessing

import warnings
warnings.filterwarnings("ignore", category=UserWarning)

# --- Import Project's Modules ---
# Import the updated GA function and LPM-related components
from deepfednas.nas.deepfednas_fitness_maximizer import \
    run_entropy_max_ga, LatencyPredictorMLP, arch_to_feature_vector_and_names, predict_latency
from deepfednas.Server.generic_server_model import GenericServerOFA
from deepfednas.utils.subnet_cost import subnet_macs
from deepfednas.data.cifar10.data_loader import load_partition_data_cifar10
from deepfednas.data.cifar100.data_loader import load_partition_data_cifar100
from deepfednas.data.cinic10.data_loader import load_partition_data_cinic10
from sklearn.preprocessing import StandardScaler # Required for the scaler object from LPM

# ==========================================================================================
# --- 1. CONFIGURATION SECTION ---
# ==========================================================================================
# --- Modify the default values in this section directly ---

# Provide a list of trained DeepFedNAS supernet checkpoint files
MODEL_PATHS = [
    "trained_models/4-stage_continued_cached_60_subnets.pt",
]

# Specify which dataset to test on ('cifar10', 'cifar100', 'cinic10')
DATASET_NAME = 'cifar10'
DATA_PATH = "data/cifar10"

# **NEW**: Base directory for all output files
OUTPUT_BASE_DIR = "evaluation/latency_prediction/cifar10_result_latency_experiments2" # Changed name for clarity

# **NEW**: Latency Predictor Model (LPM) Paths
# These paths should point to the .pth files generated by latency_predictor_pipeline.py
LPM_GPU_MODEL_PATH = "evaluation/latency_prediction/datasets_and_predictor_models_corrected_with_values_cuda/lpm_model_and_scaler_cuda_0.pth"
LPM_CPU_MODEL_PATH = "evaluation/latency_prediction/datasets_and_predictor_models_cuda_bs-1/lpm_model_and_scaler_cpu.pth" # Ensure this path is correct if TARGET_DEVICE_FOR_SEARCH is 'cpu'

# **NEW**: Target device for the GA search's latency optimization
# 'cuda' means GA will use LPM_GPU_MODEL_PATH and GPU_ID for LPM inference.
# 'cpu' means GA will use LPM_CPU_MODEL_PATH and 'cpu' for LPM inference.
TARGET_DEVICE_FOR_SEARCH = 'cuda' # or 'cpu'

# **NEW**: Latency Budgets and Fitness Weight for the GA search
# These are the constraints/objectives for the GA, not for actual evaluation.
LATENCY_GPU_BUDGET_MS = None #15.0 # Example: hard constraint of 15ms for GPU-optimized subnets. Set to None for no hard constraint.
LATENCY_CPU_BUDGET_MS = None #130.0 # Example: hard constraint of 100ms for CPU-optimized subnets. Set to None for no hard constraint.
LATENCY_FITNESS_WEIGHT = -1.0 # A negative value penalizes higher latency. Set to 0.0 to disable soft objective.
                             # Adjust this based on desired impact; -1.0 is a stronger penalty.

# Number of independent search runs for each MACs bin
NUM_SEARCH_RUNS = 1 # (N) - Increased for better statistical significance
BASE_SEED = 42

# Parameters for the Genetic Algorithm search
GA_PARAMS = {
    'pop_size': 128,
    'generations': 256,
    'mutate_p': 0.3,
}

# **GRANULARIZED MACS TARGETS**
# Define the start, end, and number of points for MACs targets
MACS_START = 0.460e9    # 0.95 Billion
MACS_END = 3.75e9      # 3.75 Billion
NUM_MACS_TARGETS = 19 # Number of discrete MACs targets to generate

# System settings
BATCH_SIZE = 1024
GPU_ID = 0 # For actual subnet evaluation on GPU

# Parameters for true latency measurement
TRUE_LATENCY_WARMUP_ITER = 5
TRUE_LATENCY_MEASURE_ITER = 10

INPUT_IMAGE_SIZE = 32 
INPUT_IMAGE_CHANNELS = 3
LATENCY_TEST_BATCH_SIZE = 1


# ==========================================================================================
# --- 2. Helper Functions ---
# ==========================================================================================

# Function to calculate number of parameters
def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)

def load_supernet_and_config(ckpt_path, device):
    print(f"Loading supernet and config from: {ckpt_path}")
    if not os.path.exists(ckpt_path):
        print(f"ERROR: Checkpoint file not found at '{ckpt_path}'. Skipping this model.")
        return None, None
    checkpoint = torch.load(ckpt_path, map_location=device, weights_only=False)
    if "arch_params" not in checkpoint:
        print(f"ERROR: 'arch_params' dictionary not found in '{ckpt_path}'. Skipping.")
        return None, None
    arch_params = checkpoint["arch_params"]
    arch_params['original_stage_base_channels'] = np.array(arch_params['original_stage_base_channels'])
    
    model = GenericServerOFA(
        arch_params=arch_params, sampling_method='all_random', num_cli_total=1
    )
    model.set_model_params(checkpoint["params"])
    print("Supernet and config loaded successfully.")
    return model.model, arch_params

def get_data_loader(dataset_name, data_path, batch_size):
    print(f"Loading {dataset_name} test dataset...")
    loader_map = {
        'cifar10': load_partition_data_cifar10, 'cifar100': load_partition_data_cifar100, 'cinic10': load_partition_data_cinic10
    }
    loader_fn = loader_map.get(dataset_name)
    if not loader_fn: raise ValueError(f"Unsupported dataset: {dataset_name}")
    _, _, _, test_data_global, _, _, _, _ = loader_fn(
        dataset_name, data_path, "hetero", 1, 1, batch_size, batch_size
    )
    return test_data_global

def evaluate_accuracy(model, test_loader, device):
    model.to(device)
    model.eval()
    correct, total = 0, 0
    with torch.no_grad():
        for images, labels in tqdm(test_loader, desc="     Evaluating Subnet", leave=False, ncols=100):
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    return (correct / total) * 100

def load_lpm_components(lpm_path, lpm_device):
    """Helper to load LPM model and scaler in the main process."""
    if not os.path.exists(lpm_path):
        print(f"Warning: LPM model not found at {lpm_path}. Latency prediction will be skipped for this target device.")
        return None, None
    
    print(f"Loading Latency Predictor MLP and Scaler from: {lpm_path} for device {lpm_device}")
    checkpoint = torch.load(lpm_path, map_location=lpm_device, weights_only=False)
    
    # Reconstruct model
    input_dim = checkpoint['input_dim']
    hidden_layers = checkpoint['hidden_layers']
    lpm_model = LatencyPredictorMLP(input_dim, hidden_layers).to(lpm_device)
    lpm_model.load_state_dict(checkpoint['model_state_dict'])
    lpm_model.eval() # Set to evaluation mode

    scaler = checkpoint['scaler']
    return lpm_model, scaler

def measure_true_latency(model, sample_input, device, num_warmup_iter=10, num_measure_iter=50):
    """
    Measures the true inference latency of a model.
    :param model: The PyTorch model (subnet).
    :param sample_input: A sample input tensor to the model.
    :param device: The device the model and input are on ('cuda' or 'cpu').
    :param num_warmup_iter: Number of iterations to warm up the GPU/CPU.
    :param num_measure_iter: Number of iterations to average latency over.
    :return: Average latency in milliseconds.
    """
    model.to(device)
    model.eval()
    sample_input = sample_input.to(device)
    
    if device == 'cuda':
        torch.cuda.synchronize()
        starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)
    else:
        starter, ender = None, None # For CPU, just use time.perf_counter

    # Warmup
    with torch.no_grad():
        for _ in range(num_warmup_iter):
            _ = model(sample_input)
            if device == 'cuda':
                torch.cuda.synchronize()

    # Measure
    timings = []
    with torch.no_grad():
        for _ in range(num_measure_iter):
            if device == 'cuda':
                starter.record()
                _ = model(sample_input)
                ender.record()
                torch.cuda.synchronize()
                curr_latency = starter.elapsed_time(ender) # Milliseconds
            else:
                start = time.perf_counter()
                _ = model(sample_input)
                end = time.perf_counter()
                curr_latency = (end - start) * 1000 # Convert to milliseconds
            timings.append(curr_latency)

    mean_latency = np.mean(timings)
    return mean_latency


# ==========================================================================================
# --- 3. Main Logic for Processing a SINGLE Model with TIMING and LATENCY ---
# ==========================================================================================

def process_single_model(model_path, eval_device):
    model_name = os.path.basename(model_path).replace('.pt', '')
    print(f"\n{'#'*60}\n# Processing Model: {model_name}\n{'#'*60}")
    
    supernet, arch_params = load_supernet_and_config(model_path, eval_device)
    if supernet is None: return

    # Determine LPM paths and settings based on TARGET_DEVICE_FOR_SEARCH
    if TARGET_DEVICE_FOR_SEARCH == 'cuda':
        lpm_path = LPM_GPU_MODEL_PATH
        latency_budget = LATENCY_GPU_BUDGET_MS
        lpm_device_for_ga = f"cuda:{GPU_ID}" if torch.cuda.is_available() else "cpu" # Use GPU for LPM if available
        print(f"GA will optimize for GPU latency, using LPM from {lpm_path}")
    elif TARGET_DEVICE_FOR_SEARCH == 'cpu':
        lpm_path = LPM_CPU_MODEL_PATH
        latency_budget = LATENCY_CPU_BUDGET_MS
        lpm_device_for_ga = "cpu"
        print(f"GA will optimize for CPU latency, using LPM from {lpm_path}")
    else:
        raise ValueError(f"Unknown TARGET_DEVICE_FOR_SEARCH: {TARGET_DEVICE_FOR_SEARCH}")

    # Load LPM for *this main process* to be passed to the GA initializer.
    # The `lpm_model_combined_path` arg to `run_entropy_max_ga` will point to `lpm_path`.
    # The `predict_latency` function used *outside* of the GA will also use this `lpm_model_main` and `scaler_main`.
    lpm_model_main, scaler_main = load_lpm_components(lpm_path, lpm_device_for_ga) # This is the LPM used for the GA process itself

    test_loader = get_data_loader(DATASET_NAME, DATA_PATH, BATCH_SIZE)
    # Get a sample input for true latency measurement
    sample_input, _ = next(iter(test_loader))
    if BATCH_SIZE >= LATENCY_TEST_BATCH_SIZE:
        sample_input = sample_input[0:LATENCY_TEST_BATCH_SIZE] 

    depth_choices = np.array(list(range(arch_params['max_extra_blocks_per_stage'] + 1)))
    width_choices = arch_params['width_multiplier_choices']
    exp_choices = np.array(arch_params['expansion_ratio_choices'])

    results_data = [] # For mean_accuracy, std_dev_accuracy per bin
    subnet_details_data = [] # For individual subnet details: MACs, Params, Accuracy, Arch, Latency, True Latency

    # Ensure output directory exists
    os.makedirs(OUTPUT_BASE_DIR, exist_ok=True)
    
    # Define output file for detailed subnet info
    clean_model_name = model_name.replace('.', '_').replace('/', '_')
    # Add target device to filename for clarity
    subnet_details_output_path = os.path.join(OUTPUT_BASE_DIR, f"deepfednas_subnet_details_{clean_model_name}_{DATASET_NAME}_target_{TARGET_DEVICE_FOR_SEARCH}.csv")
    unified_results_output_path = os.path.join(OUTPUT_BASE_DIR, f"deepfednas_summary_results_{clean_model_name}_{DATASET_NAME}_target_{TARGET_DEVICE_FOR_SEARCH}.csv")
    timing_summary_output_path = os.path.join(OUTPUT_BASE_DIR, f"deepfednas_timing_summary_{clean_model_name}_{DATASET_NAME}_target_{TARGET_DEVICE_FOR_SEARCH}.json")


    start_time_total_search = time.time()
    
    # **GRANULARIZED MACS TARGETS GENERATION**
    macs_targets = np.linspace(MACS_START, MACS_END, NUM_MACS_TARGETS)
    
    for i, macs_target in enumerate(macs_targets):
        bin_name = f"MACs_{macs_target/1e9:.2f}B"
        print(f"\n{'='*30}\nProcessing MACs Target: {bin_name} (Constraint: {macs_target/1e9:.2f}B)\n{'='*30}")
        bin_accuracies = []
        bin_predicted_latencies = [] # Store predicted latencies for average/std
        bin_true_latencies = []    # Store true latencies for average/std
        
        for run_idx in range(NUM_SEARCH_RUNS):
            run_seed = BASE_SEED + run_idx
            print(f"\n--- Run {run_idx+1}/{NUM_SEARCH_RUNS} (Seed: {run_seed}) ---")
            
            best_arch, _ = run_entropy_max_ga(
                mac_budget=macs_target,
                arch_config_params=arch_params,
                width_mult_options=width_choices,
                depth_choices=depth_choices,
                exp_opt_values=exp_choices,
                rho0_constraint=arch_params.get('supernet_rho0_constraint', 2.0),
                effectiveness_fitness_weight=arch_params.get('supernet_effectiveness_fitness_weight', 100),
                pop_size=GA_PARAMS['pop_size'],
                generations=GA_PARAMS['generations'],
                mutate_p=GA_PARAMS['mutate_p'],
                seed=run_seed,
                
                # Latency-specific parameters passed to GA
                lpm_model_combined_path=lpm_path, # Path to the LPM for workers to load
                latency_budget_ms=latency_budget,
                latency_fitness_weight=LATENCY_FITNESS_WEIGHT,
                lpm_device=lpm_device_for_ga # Device for LPM inference inside workers
            )
            
            if best_arch:
                exp_indices = [list(exp_choices).index(val) for val in best_arch['e']]
                supernet.set_active_subnet(d=best_arch['d'], e_indices=exp_indices, w_indices=best_arch['w_indices'])
                static_subnet = supernet.get_active_subnet(preserve_weight=True)
                
                num_params = count_parameters(static_subnet)
                accuracy = evaluate_accuracy(static_subnet, test_loader, eval_device) # eval_device for accuracy
                actual_macs, _ = subnet_macs(best_arch['d'], best_arch['e'], best_arch['w_indices'], arch_params['width_multiplier_choices'], arch_params)

                predicted_latency = -1.0 # Default if LPM is not used or fails
                if lpm_model_main and scaler_main: # Use LPM loaded in main process for this
                    predicted_latency = predict_latency(best_arch, arch_params, lpm_model_main, scaler_main, lpm_device_for_ga)

                # --- Measure TRUE Latency ---
                true_latency = -1.0
                print(f"     Measuring true latency on {TARGET_DEVICE_FOR_SEARCH}...")
                try:
                    true_latency = measure_true_latency(static_subnet, sample_input, TARGET_DEVICE_FOR_SEARCH,
                                                        num_warmup_iter=TRUE_LATENCY_WARMUP_ITER,
                                                        num_measure_iter=TRUE_LATENCY_MEASURE_ITER)
                    print(f"     True Latency ({eval_device}): {true_latency:.2f}ms")
                except Exception as e:
                    print(f"     Error measuring true latency: {e}")

                print(f"Run {run_idx+1} Best Arch Accuracy: {accuracy:.2f}% (MACs: {actual_macs/1e9:.2f}B, Params: {num_params/1e6:.2f}M, Pred. Latency ({TARGET_DEVICE_FOR_SEARCH}): {predicted_latency:.2f}ms, True Latency ({eval_device}): {true_latency:.2f}ms)")
                
                bin_accuracies.append(accuracy)
                bin_predicted_latencies.append(predicted_latency)
                bin_true_latencies.append(true_latency)

                subnet_details_data.append({
                    'model_name': model_name,
                    'macs_target_bin': bin_name,
                    'macs_constraint': macs_target,
                    'run_seed': run_seed,
                    'actual_macs': actual_macs,
                    'num_parameters': num_params,
                    'test_accuracy': accuracy,
                    'predicted_latency_ms': predicted_latency,
                    'true_latency_ms': true_latency, # NEW
                    'arch_d': str(best_arch['d']),
                    'arch_e': str(best_arch['e']),
                    'arch_w_indices': str(best_arch['w_indices'])
                })
            else:
                print(f"Warning: GA search for Run {run_idx+1} did not find a valid architecture for MACs constraint {macs_target/1e9:.2f}B.")
            
            if eval_device.type == 'cuda':
                torch.cuda.empty_cache()
                # Optional: Force garbage collection to help Python free objects that might still hold CUDA memory
                import gc
                gc.collect() 

        if bin_accuracies:
            mean_acc, std_acc = np.mean(bin_accuracies), np.std(bin_accuracies)
            mean_pred_latency, std_pred_latency = np.mean(bin_predicted_latencies), np.std(bin_predicted_latencies)
            mean_true_latency, std_true_latency = np.mean(bin_true_latencies), np.std(bin_true_latencies) # NEW
            
            print(f"\nTarget '{bin_name}' Summary: Accuracy = {mean_acc:.2f} ± {std_acc:.2f}% (Pred. Latency = {mean_pred_latency:.2f} ± {std_pred_latency:.2f} ms, True Latency = {mean_true_latency:.2f} ± {std_true_latency:.2f} ms)")
            results_data.append({
                'macs_target_bin': bin_name,
                'macs_constraint': macs_target,
                'mean_accuracy': mean_acc,
                'std_dev_accuracy': std_acc,
                'mean_predicted_latency_ms': mean_pred_latency,
                'std_dev_predicted_latency_ms': std_pred_latency,
                'mean_true_latency_ms': mean_true_latency, # NEW
                'std_dev_true_latency_ms': std_true_latency # NEW
            })
        else:
            print(f"Target '{bin_name}' failed to produce any valid results over {NUM_SEARCH_RUNS} runs.")
            results_data.append({
                'macs_target_bin': bin_name,
                'macs_constraint': macs_target,
                'mean_accuracy': 0.0,
                'std_dev_accuracy': 0.0,
                'mean_predicted_latency_ms': -1.0,
                'std_dev_predicted_latency_ms': 0.0,
                'mean_true_latency_ms': -1.0, # NEW
                'std_dev_true_latency_ms': 0.0 # NEW
            })
        
        if eval_device.type == 'cuda':
            torch.cuda.empty_cache()
            import gc
            gc.collect()

    total_search_and_eval_time_sec = time.time() - start_time_total_search
    total_search_and_eval_time_min = total_search_and_eval_time_sec / 60
    print(f"\nTotal search and evaluation time for this model: {total_search_and_eval_time_sec:.2f} seconds ({total_search_and_eval_time_min:.2f} minutes).")
    
    # --- Save Summary Results to a CSV ---
    df_results_summary = pd.DataFrame(results_data)
    df_results_summary.to_csv(unified_results_output_path, index=False)
    print(f"\nSummary results for {model_name} (Target: {TARGET_DEVICE_FOR_SEARCH}) saved to: {unified_results_output_path}")

    # --- Save Detailed Subnet Information to a CSV ---
    df_subnet_details = pd.DataFrame(subnet_details_data)
    df_subnet_details.to_csv(subnet_details_output_path, index=False)
    print(f"Detailed subnet information for {model_name} (Target: {TARGET_DEVICE_FOR_SEARCH}) saved to: {subnet_details_output_path}")

    # --- Save Timing Information to a JSON ---
    timing_data = {
        'model_name': model_name,
        'dataset_name': DATASET_NAME,
        'target_device_for_search': TARGET_DEVICE_FOR_SEARCH,
        'total_search_and_evaluation_time_seconds': total_search_and_eval_time_sec,
        'total_search_and_evaluation_time_minutes': total_search_and_eval_time_min,
        'num_macs_targets': NUM_MACS_TARGETS,
        'num_search_runs_per_target': NUM_SEARCH_RUNS,
        'ga_pop_size': GA_PARAMS['pop_size'],
        'ga_generations': GA_PARAMS['generations']
    }
    with open(timing_summary_output_path, 'w') as f:
        json.dump(timing_data, f, indent=4)
    print(f"Timing summary for {model_name} (Target: {TARGET_DEVICE_FOR_SEARCH}) saved to: {timing_summary_output_path}")


# ==========================================================================================
# --- 4. Main Execution Loop ---
# ==========================================================================================

if __name__ == "__main__":
    multiprocessing.set_start_method('spawn', force=True)
    print(f"--- Phase 1: DeepFedNAS Principled Search and Evaluation (Batch Mode) ---")
    eval_device = torch.device(f"cuda:{GPU_ID}" if torch.cuda.is_available() and GPU_ID >= 0 else "cpu")
    print(f"Subnet evaluation (accuracy measurement and true latency) will run on: {eval_device}")
    
    for model_path in MODEL_PATHS:
        process_single_model(model_path, eval_device)
        
    print(f"\n{'='*30}\nAll DeepFedNAS models processed!\n{'='*30}")